# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12D4IT12cIeU-g-zZaXYWt4PMEBkt0BCs
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Project

# Commented out IPython magic to ensure Python compatibility.
# %ls

!pip install pyswarms
!pip install pyswarm

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from pyswarms.single import GlobalBestPSO
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
import warnings
warnings.filterwarnings('ignore')

# Data Loading and Preprocessing
df = pd.read_csv('Train_data.csv', header=None, low_memory=False)
df.columns = df.iloc[0]
df = df[1:]
df.head()

le = LabelEncoder()
categorical_cols = ['protocol_type', 'service', 'flag']
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])
df.head()

# Define columns to exclude from attack
EXCLUDED_COLS = ['protocol_type', 'flag', 'land', 'hot', 'root_shell', 'num_outbound_cmds']
X = df.drop(['class'] + EXCLUDED_COLS, axis=1)
excluded_data = df[EXCLUDED_COLS]
y = le.fit_transform(df['class'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Train model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Evaluation function
def evaluate_model(model, X, y):
    y_pred = model.predict(X)
    print(f"Accuracy: {accuracy_score(y, y_pred):.4f}")
    print(classification_report(y, y_pred))
    cm = confusion_matrix(y, y_pred)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.show()

# Initial evaluation
print("Original Model Performance:")
evaluate_model(rf, X_test, y_test)

# Feature importance
importances = rf.feature_importances_
features = X.columns
plt.barh(features[np.argsort(importances)[-20:]], importances[np.argsort(importances)[-20:]])
plt.title("Feature Importance")
plt.show()

# Ensure all features are numeric
X_train = X_train.apply(pd.to_numeric, errors='coerce')
X_test = X_test.apply(pd.to_numeric, errors='coerce')
X_train = X_train.dropna(axis=1)
X_test = X_test.dropna(axis=1)

# Update feature names
features = X_train.columns

# PSO-based attack
def pso_attack(model, X_test, y_test):
    # PSO configuration
    options = {'c1': 0.7, 'c2': 0.4, 'w': 0.8}
    n_particles = 50
    max_iter = 200
    max_perturb = 5.0

    bounds = (np.full(X_test.shape[1], -max_perturb),
             np.full(X_test.shape[1], max_perturb))

    # Adversarial objective function
    def objective(perturbations):
        perturb_3d = perturbations.reshape((n_particles, 1, X_test.shape[1]))

        # Create perturbed dataset (n_particles, n_samples, n_features)
        perturbed_data = X_test + perturb_3d
        preds = model.predict(perturbed_data.reshape(-1, X_test.shape[1]))
        preds = preds.reshape(n_particles, -1)
        return np.mean(preds[:, y_test == 1] == 0, axis=1)

    # Initialize and run PSO
    optimizer = GlobalBestPSO(n_particles=n_particles,
                            dimensions=X_test.shape[1],
                            options=options,
                            bounds=bounds)
    cost, pos = optimizer.optimize(objective, iters=max_iter)

    return pos

# Generate adversarial perturbations
print("\nGenerating adversarial perturbations with PSO...")
best_perturbation = pso_attack(rf, X_test.values, y_test)

# Create adversarial dataset
X_adv = X_test.copy()
X_adv.iloc[:, :] = X_test.values + best_perturbation

# Save adversarial dataset
adv_filename = 'adversarial_dataset.csv'
X_adv.to_csv(adv_filename, index=False)
print(f"\nAdversarial dataset saved as {adv_filename}")

# Evaluate original model on adversarial data
print("\nEvaluating Original Model on Adversarial Data:")
evaluate_model(rf, X_adv, y_test)

# Feature importance after attack
importances_adv = rf.feature_importances_
plt.barh(features[np.argsort(importances_adv)[-20:]],
        importances_adv[np.argsort(importances_adv)[-20:]])
plt.title("Feature Importance After Attack")
plt.show()

# Perturbation analysis
perturbation_analysis = pd.DataFrame({
    'Feature': features,
    'Perturbation': best_perturbation,
    'Absolute_Perturbation': np.abs(best_perturbation)
}).sort_values('Absolute_Perturbation', ascending=False)

print("\nTop 10 Most Perturbed Features:")
print(perturbation_analysis.head(10))

# Train  model on adversarial data
print("\nTraining New Model on Adversarial Data...")
X_combined = pd.concat([X_train, X_adv])
y_combined = np.concatenate([y_train, y_test])
rf_adv = RandomForestClassifier(n_estimators=100, random_state=42)
rf_adv.fit(X_combined, y_combined)

# Evaluate new model
print("\nAdversarial Model Evaluation:")
evaluate_model(rf_adv, X_test, y_test)

# Autoencoder-based Adversarial Detection
def train_adversarial_detector(X_clean):
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X_clean)

    input_dim = X_scaled.shape[1]
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(64, activation='relu')(input_layer)
    decoded = Dense(input_dim, activation='sigmoid')(encoded)
    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer=Adam(0.001), loss='mse')

    autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, verbose=0)

    reconstructions = autoencoder.predict(X_scaled, verbose=0)
    mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
    threshold = np.percentile(mse, 95)

    return autoencoder, threshold, scaler

print("\nTraining Autoencoder-based Adversarial Detector...")
detector, threshold, scaler = train_adversarial_detector(X_train)

# Adversarial Training
def adversarial_training(model, X_train, y_train, X_adv, y_adv, epochs=5):
    for epoch in range(epochs):
        X_combined = np.vstack([X_train, X_adv])
        y_combined = np.hstack([y_train, y_adv])
        model.fit(X_combined, y_combined)
        print(f"Epoch {epoch + 1}/{epochs} complete.")
    return model

y_adv = np.zeros(X_adv.shape[0])
print("\nTraining Robust Model with Adversarial Training...")
rf_robust = RandomForestClassifier(n_estimators=100, random_state=42)
rf_robust = adversarial_training(rf_robust, X_train, y_train, X_adv, y_adv)

print("\nEvaluating Robust Model on Adversarial Dataset:")
y_pred_robust = rf_robust.predict(X_adv)
print(f"Accuracy: {accuracy_score(y_test, y_pred_robust):.4f}")
print(classification_report(y_test, y_pred_robust))

# Autoencoder Defense Mechanism
def defend_against_attack(model, detector, threshold, scaler, X_test):
    X_scaled = scaler.transform(X_test)
    reconstructions = detector.predict(X_scaled, verbose=0)
    mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
    is_adversarial = mse > threshold

    if np.any(is_adversarial):
        print(f"WARNING: {np.sum(is_adversarial)} adversarial samples detected!")
        clean_samples = X_test[~is_adversarial]
        if clean_samples.shape[0] > 0:
            return model.predict(clean_samples), is_adversarial
        else:
            return None, is_adversarial
    else:
        return model.predict(X_test), is_adversarial

print("\nTesting Defense Mechanism on Adversarial Dataset:")
y_pred_defended, is_adversarial = defend_against_attack(rf, detector, threshold, scaler, X_adv)

if y_pred_defended is not None:
    clean_indices = ~is_adversarial
    print("\nModel Performance on Clean Samples:")
    print(f"Accuracy: {accuracy_score(y_test[clean_indices], y_pred_defended):.4f}")
    print(classification_report(y_test[clean_indices], y_pred_defended))